# Vector Protocol

Protocol is where the core Vector framework is defined. Protocol takes in method calls over a simple JSON RPC interface, uses them to make updates to its shared state with a channel peer, and stores resulting commmitments.

Contents:

- [Developing and Running Tests](https://github.com/connext/vector/tree/master/modules/protocol#developing-and-running-tests)
- [Core Principles](https://github.com/connext/vector/tree/master/modules/protocol#core-principles)
- [Sync Protocol](https://github.com/connext/vector/tree/master/modules/protocol#sync)
- [Update Types](https://github.com/connext/vector/tree/master/modules/protocol#update-types)
- [RPC Interface Reference] // TODO

## Developing and Running Tests

In `~/vector` (root), run:

- `make` to build everything including the protocol
- `make test-protocol` to run the tests

## Core Principles

Vector uses the following fundamental design principles:

- All channels have two parties
- Channel participants[] are ordered by `initiator, responder`
- The protocol _only_ allows for single-turn conditional transfers. The logic for resolving the transfer can have arbitrary complexity so long as it conforms to a base standard around how `balances` are defined. Because the logic can be complex (though only have single "turn" played by the receiver), it is possible to theoretically construct many-turn State Channel applications so long as each state is independently finalizeable. The communication for these "meta"-updates MUST happen entirely out-of-band and be validated by the implementer independently of the protocol.
- Vector uses leader-election/consensus for concurrency control (like CF), rather than using CRDTs like StateChannels

![alt text](https://i.ibb.co/J2cT0dG/Vector-Phases.png)

The core protocol flow has three primary phases:

1. **Leader Election** -- this is done using a distributed lock implementation. Peers queue updates on the lock and execute them serially.
2. **Update Generation** -- a proposed update is generated by the sender in-memory
3. **Syncronization** -- the sender's update is dispatched over the wire. Receiver validates the update, merges the update with their channel, stores the channel, and then acks. Sender receives the ack and stores.

## Sync Protocol

At the core of Vector lies the `sync` protocol. Unlike in CounterFactual, there is only a single protocol -- `sync` is used both when a sender wants to propose a new update to the replicated state, and _also_ when peer state has diverged. Because updates are monotonic (nonce += 1), there are only a very limited number of cases within which party states possibly diverge.

For this reason, `sync` takes a higher degree of control over message delivery than CF does. Outbound messages are retried on a timer if they fail, inbound onces are idempotent. Higher-nonced inbound messages are checked against the `ChannelState` latest nonce and dropped if they are duplicates, or saved to store if they aren't.

## Update Types

All channel updates fall into one of 4 types. Each update type is responsible for generating and storing one [double-signed commitment](https://github.com/connext/vector/blob/master/modules/contracts/README.md#commitments).

Note that there is no specific update for `Withdraw`. That is because a withdraw op can be constructed in an easy and generalizeable way using `Create` and `Resolve` similar to what we currently do in CF.

### Setup

Like in CF, creating a new channel simply involves calculating a `channelId` which is the CREATE2 address at which a proxy to the `Multisig.sol` contract will be deployed.

After that, Alice and Bob sign a `0` nonce update which sets up the channel state between them. This type of update is only ever needed once. Unlike in CF, `setup` does not actually sign a unique commitment, but instead signs a `ChannelCommitment` just like all of the other updates. Theoretically, this means that we don't actually _need_ a setup-specific commitment. However, it can be desirable to do this so that a peer can know that a new connection has been created that they can choose to accept.

Setup also performs one more highly critical task -- the channel initiator/responder, as defined in `participants[]`, are mutually agreed upon. This is a necessary component of how deposits should occur.

### Deposit

A deposit update should occur after deposits have been send to chain (either by calling the `depositA` function for the channel initiator, or simply sending funds to the multisig for the channel responder).

The deposit update is used to confirm an already-mined deposit tx into the channel `balance`. To do this safely, the following must occur:

1. The update initiator's balance must be incremented by the deposit amount (calculating new balances for each party using onchain data as described in the [Funding a Channel](https://github.com/connext/vector/blob/master/modules/contracts/README.md#funding-a-channel) writeup). Note that this is per-assetAddress, so a new assetAddress may need to be added to the `assetAddress` array.
2. The channel nonce must be updated by 1.
3. The `latestDepositNonce` in state must be set to whatever is onchain for Alice.
4. A new `ChannelCommitment` must be generated using the above and signed by both parties.
5. Set this update to `state.latestUpdate`.

### Create

A create update should occur when both parties want to create a new conditional transfer.

The create update must do the following:

1. Decrement the channel state `balance` on one (or both) sides by the amount that will be locked in the transfer (indexed by assetAddress).
2. Increase the `lockedBalance` in the channel state by the total locked maount of value (indexed by assetAddress).
3. Update the channel nonce by 1.
4. Create a new `TransferState` with a status of CREATED, passing in the correct params.
5. Hash the `TransferState` and add it to the merkle root in the new channel state.
6. Generate a duoblesigned `ChannelCommitment`.
7. Set this update to `state.latestUpdate`.

### Resolve

A resolve update should occur when both parties want to resolve a conditional transfer and reintroduce it's balances back to the main channel balance.

Should do the exact oppositve of the `create` update above.

## Protocol TODOs

#### Vector.ts

- [ ] Add try/catch around everything inside lock so it always releases the lock instead of timing it out -- or whatever, we just **need** to release the lock!!!
      NOTE: Currently done via the `Result` type, do we want to change this?
- [x] Move validation (including checking if StateChannel exists) inside the lock
- [x] Pass channel state into `generateUpdate` and `sync.outbound` to reduce a couple of db queries.
- [x] Remove posting outbound evt event in `executeUpdate`
- [x] Add sync on connect for now, we can take it out later if connecting takes too long.
- [x] Add exception case for null channel for setup
- [x] Remove `amount` from depositParams.

#### Update.ts

- [ ] Check onchain deposit balance in `generateDepositUpdate`
- [x] Calculate `transferId` programmatically using `nonce`, `channelAddress`, `transferDefinition`, `transferTimeout`, etc. (just check how we do it for appIdHash) -- note that we need to validate that this is derived correctly on responder side
- [x] Verify hashing + signing functions with what is onchain
- [x] Fix ordering of `to` in `getUpdatedChannelBalance` (and verify that transfer/channel `to` ordering _can_ be different elsewhere)

#### Validate.ts

- [ ] Validate timeout >= minimum timeout from types/constants
- [ ] Implement as comments/methods

#### Create2

- [ ] Tell heiko to keep `proxyFactory.proxyCreationCode()` in the ChannelFactory. If we don't allow this to be called from the contract, there's a lot more storage overhead to deal with old bytecode.

#### Types

- [x] Remove encodings from core transfer type
- [ ] Review `FullTransferState` type

#### Other

- [x] Replace `merkleTree.ts` with external dep
